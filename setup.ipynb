{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f80343",
   "metadata": {},
   "source": [
    "## Install Ollama\n",
    "\n",
    "Download & Install Ollama:\n",
    "[ollama.ai](https://ollama.ai)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67393530",
   "metadata": {},
   "source": [
    "## Install the LLM\n",
    "\n",
    "```bash\n",
    "# Go Crazy: 43GB\n",
    "ollama pull llama3.1:8b\n",
    "\n",
    "# Test your setup\n",
    "ollama run llama3.1:8b \"Hello! Test my local AI setup for coding and agent tasks\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847d137",
   "metadata": {},
   "source": [
    "## Create Virtual Environment\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    ".venv\\Scripts\\activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc5c83",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "```bash\n",
    "python -m pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "python -m ipykernel install --user --name=venv --display-name=\"Google ADK Multi-Provider\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35899f8c",
   "metadata": {},
   "source": [
    "## Create .env for API\n",
    "\n",
    "```bash\n",
    "\n",
    "# LOCAL DEVELOPMENT (RECOMMENDED FOR LEARNING)\n",
    "OLLAMA_BASE_URL=http://localhost:11434\n",
    "OLLAMA_MODEL=llama3.2:70b\n",
    "USE_LOCAL_MODELS=true\n",
    "\n",
    "# SMART PROVIDER SELECTION\n",
    "PREFERRED_PROVIDER=local  # Start FREE!\n",
    "FALLBACK_PROVIDER=google\n",
    "\n",
    "# COST CONTROL\n",
    "MONTHLY_BUDGET_LIMIT=50.00\n",
    "COST_TRACKING_ENABLED=true\n",
    "ALERT_THRESHOLD_PERCENT=80\n",
    "\n",
    "# DEVELOPMENT SETTINGS\n",
    "DEBUG=true\n",
    "TEMPERATURE=0.7\n",
    "MAX_TOKENS=1000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbbd680",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45434356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'virtual_env_active': True, '..\\requirements.txt': False, '..\\\\.env.example': True, '..\\\\.env': True}\n"
     ]
    }
   ],
   "source": [
    "def validate_setup(current_dir, essential_files, in_venv):\n",
    "    results = {}\n",
    "    results['virtual_env_active'] = in_venv\n",
    "\n",
    "    for file_path in essential_files.keys():\n",
    "        # Use only the file name, not parent directory\n",
    "        full_path = (current_dir / Path(file_path).name).resolve()\n",
    "        results[file_path] = full_path.exists()\n",
    "\n",
    "    return results\n",
    "\n",
    "validation_results = validate_setup(current_dir, essential_files, in_venv)\n",
    "print(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3a41f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE MULTI-PROVIDER DEPENDENCIES CHECK\n",
      "==================================================\n",
      "\n",
      " Core Framework:\n",
      " google.adk.agents    - Google ADK Framework\n",
      " dotenv               - Environment Management\n",
      " pydantic             - Data Validation & Settings\n",
      " Core Framework: Complete (3/3)\n",
      "\n",
      " Local AI (FREE):\n",
      " ollama               - Local Model Integration (Llama 3.2)\n",
      " Local AI (FREE): Complete (1/1)\n",
      "\n",
      " Major Cloud Providers:\n",
      " google.generativeai  - Google AI Studio\n",
      " openai               - OpenAI Integration\n",
      " anthropic            - Anthropic Integration\n",
      " Major Cloud Providers: Complete (3/3)\n",
      "\n",
      " Additional Cloud Providers:\n",
      " groq                 - Groq Fast Inference\n",
      " together             - Together AI Platform\n",
      " mistralai            - Mistral AI Platform\n",
      " cohere               - Cohere Platform\n",
      " Additional Cloud Providers: Complete (4/4)\n",
      "\n",
      " Multi-Provider Interface:\n",
      " litellm              - Universal LLM Interface\n",
      " requests             - HTTP Client\n",
      " httpx                - Modern HTTP Client\n",
      " Multi-Provider Interface: Complete (3/3)\n",
      "\n",
      " Development Environment:\n",
      " jupyter              - Jupyter Notebooks\n",
      " ipykernel            - Kernel Management\n",
      " rich                 - Enhanced Terminal Output\n",
      " Development Environment: Complete (3/3)\n",
      "\n",
      "üìä Overall Installation Status: 17/17 packages\n",
      " ALL DEPENDENCIES INSTALLED!\n",
      " Complete multi-provider development environment ready\n",
      " You can use ANY supported LLM provider\n",
      "\n",
      " Next: Configure your preferred providers in .env file\n"
     ]
    }
   ],
   "source": [
    "# Complete Multi-Provider Dependencies Verification\n",
    "print(\"COMPLETE MULTI-PROVIDER DEPENDENCIES CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Comprehensive dependencies for multi-provider support\n",
    "dependencies = {\n",
    "    'Core Framework': {\n",
    "        'google.adk.agents': 'Google ADK Framework',\n",
    "        'dotenv': 'Environment Management',\n",
    "        'pydantic': 'Data Validation & Settings'\n",
    "    },\n",
    "    'Local AI (FREE)': {\n",
    "        'ollama': 'Local Model Integration (Llama 3.2)',\n",
    "    },\n",
    "    'Major Cloud Providers': {\n",
    "        'google.generativeai': 'Google AI Studio',\n",
    "        'openai': 'OpenAI Integration',\n",
    "        'anthropic': 'Anthropic Integration'\n",
    "    },\n",
    "    'Additional Cloud Providers': {\n",
    "        'groq': 'Groq Fast Inference',\n",
    "        'together': 'Together AI Platform',\n",
    "        'mistralai': 'Mistral AI Platform',\n",
    "        'cohere': 'Cohere Platform'\n",
    "    },\n",
    "    'Multi-Provider Interface': {\n",
    "        'litellm': 'Universal LLM Interface',\n",
    "        'requests': 'HTTP Client',\n",
    "        'httpx': 'Modern HTTP Client'\n",
    "    },\n",
    "    'Development Environment': {\n",
    "        'jupyter': 'Jupyter Notebooks',\n",
    "        'ipykernel': 'Kernel Management',\n",
    "        'rich': 'Enhanced Terminal Output'\n",
    "    }\n",
    "}\n",
    "\n",
    "total_deps = sum(len(deps) for deps in dependencies.values())\n",
    "installed_count = 0\n",
    "missing_deps = []\n",
    "\n",
    "for category, deps in dependencies.items():\n",
    "    print(f\"\\n {category}:\")\n",
    "    category_installed = 0\n",
    "    category_total = len(deps)\n",
    "    \n",
    "    for package, description in deps.items():\n",
    "        try:\n",
    "            # Handle special import cases\n",
    "            if package == 'google.adk.agents':\n",
    "                from google.adk.agents import Agent\n",
    "            elif package == 'dotenv':\n",
    "                from dotenv import load_dotenv\n",
    "            elif package == 'mistralai':\n",
    "                import mistralai\n",
    "            else:\n",
    "                # Standard import\n",
    "                __import__(package.replace('-', '_'))\n",
    "            \n",
    "            print(f\" {package:<20} - {description}\")\n",
    "            installed_count += 1\n",
    "            category_installed += 1\n",
    "        except ImportError:\n",
    "            print(f\" {package:<20} - {description} (MISSING)\")\n",
    "            missing_deps.append(package)\n",
    "    \n",
    "    # Category summary\n",
    "    if category_installed == category_total:\n",
    "        print(f\" {category}: Complete ({category_installed}/{category_total})\")\n",
    "    elif category_installed > 0:\n",
    "        print(f\" {category}: Partial ({category_installed}/{category_total})\")\n",
    "    else:\n",
    "        print(f\" {category}: Missing ({category_installed}/{category_total})\")\n",
    "\n",
    "print(f\"\\nüìä Overall Installation Status: {installed_count}/{total_deps} packages\")\n",
    "\n",
    "# Smart recommendations based on what's missing\n",
    "if installed_count == total_deps:\n",
    "    print(\" ALL DEPENDENCIES INSTALLED!\")\n",
    "    print(\" Complete multi-provider development environment ready\")\n",
    "    print(\" You can use ANY supported LLM provider\")\n",
    "    \n",
    "elif installed_count >= (total_deps * 0.7):  # 70% or more\n",
    "    print(\" Core dependencies ready - mostly functional\")\n",
    "    print(f\" {len(missing_deps)} optional packages missing\")\n",
    "    print(\" You can proceed and add providers as needed\")\n",
    "    \n",
    "else:\n",
    "    print(\" Critical packages missing\")\n",
    "    print(\" Course environment not ready\")\n",
    "    print(\"\\n COMPLETE FIX:\")\n",
    "    print(\"1. Ensure virtual environment is activated:\")\n",
    "    print(\"   Windows: .venv\\\\Scripts\\\\activate\")\n",
    "    print(\"   Mac/Linux: source .venv/bin/activate\")\n",
    "    print(\"2. Install all dependencies:\")\n",
    "    print(\"   pip install -r requirements.txt\")\n",
    "    print(\"3. Restart Jupyter kernel\")\n",
    "    print(\"4. Re-run this cell\")\n",
    "\n",
    "print(\"\\n Next: Configure your preferred providers in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e97a4abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI-PROVIDER CONFIGURATION TEST\n",
      "=============================================\n",
      " Environment Configuration:\n",
      " Local Models Enabled: ‚úÖ YES\n",
      " Preferred Provider: local\n",
      " Configured for FREE local development (recommended!)\n",
      " Ollama URL: http://localhost:11434\n",
      " Default Model: llama3.2:8b\n",
      " Ollama running with 5 model(s):\n",
      " llama3.1:8b (PERFECT for coding/agents!)\n",
      " qwen3:latest\n",
      " gemma3:latest\n",
      " Llama 3.1 detected - optimized for enterprise agent tasks!\n",
      "\n",
      " Cloud Provider Status:\n",
      "üí§ Google     - Not configured (optional)\n",
      "üí§ OpenAI     - Not configured (optional)\n",
      "üí§ Anthropic  - Not configured (optional)\n",
      "üí§ Groq       - Not configured (optional)\n",
      "üí§ Deepseek   - Not configured (optional)\n",
      "\n",
      " Setup Assessment:\n",
      " Ready for FREE local development\n",
      " You can start building immediately with zero costs\n",
      " Llama 3.1 provides enterprise-grade capabilities for agent development\n",
      "\n",
      " Next: Test Google ADK framework\n"
     ]
    }
   ],
   "source": [
    "# Multi-Provider Configuration Test\n",
    "print(\"MULTI-PROVIDER CONFIGURATION TEST\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    \n",
    "    # Load environment configuration\n",
    "    load_dotenv()\n",
    "    \n",
    "    print(\" Environment Configuration:\")\n",
    "    \n",
    "    # Check local setup (preferred)\n",
    "    use_local = os.getenv('USE_LOCAL_MODELS', 'true').lower() == 'true'\n",
    "    preferred_provider = os.getenv('PREFERRED_PROVIDER', 'local')\n",
    "    \n",
    "    print(f\" Local Models Enabled: {'‚úÖ YES' if use_local else '‚ùå NO'}\")\n",
    "    print(f\" Preferred Provider: {preferred_provider}\")\n",
    "    \n",
    "    if use_local and preferred_provider == 'local':\n",
    "        print(\" Configured for FREE local development (recommended!)\")\n",
    "        \n",
    "        # Test Ollama connection\n",
    "        ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
    "        ollama_model = os.getenv('OLLAMA_MODEL', 'llama3.1:1b')\n",
    "        print(f\" Ollama URL: {ollama_url}\")\n",
    "        print(f\" Default Model: {ollama_model}\")\n",
    "        \n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(f\"{ollama_url}/api/tags\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                models = response.json().get('models', [])\n",
    "                if models:\n",
    "                    print(f\" Ollama running with {len(models)} model(s):\")\n",
    "                    for model in models[:3]:  # Show first 3\n",
    "                        model_name = model['name']\n",
    "                        if 'llama3.1' in model_name:\n",
    "                            print(f\" {model_name} (PERFECT for coding/agents!)\")\n",
    "                        else:\n",
    "                            print(f\" {model_name}\")\n",
    "                    \n",
    "                    # Check if recommended model is available\n",
    "                    available_models = [m['name'] for m in models]\n",
    "                    if any('llama3.1' in name for name in available_models):\n",
    "                        print(\" Llama 3.1 detected - optimized for enterprise agent tasks!\")\n",
    "                    else:\n",
    "                        print(\" Consider installing Llama 3.1: ollama pull llama3.1:1b\")\n",
    "                else:\n",
    "                    print(\" Ollama running but no models installed\")\n",
    "                    print(\" Install Llama 3.1: ollama pull llama3.1:1b\")\n",
    "            else:\n",
    "                print(\"‚ùå Ollama not responding\")\n",
    "                print(\" Start Ollama service and install models\")\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Cannot connect to Ollama\")\n",
    "            print(\" Start Ollama: Visit ollama.ai for installation\")\n",
    "            print(f\"   Error details: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Check cloud provider configurations (optional)\n",
    "    print(\"\\n Cloud Provider Status:\")\n",
    "    \n",
    "    cloud_providers = {\n",
    "        'Google': 'GOOGLE_API_KEY',\n",
    "        'OpenAI': 'OPENAI_API_KEY', \n",
    "        'Anthropic': 'ANTHROPIC_API_KEY',\n",
    "        'Groq': 'GROQ_API_KEY',\n",
    "        'Deepseek': 'DEEPSEEK_API_KEY'\n",
    "    }\n",
    "    \n",
    "    configured_providers = []\n",
    "    for provider, key_name in cloud_providers.items():\n",
    "        api_key = os.getenv(key_name)\n",
    "        if api_key and api_key != f'your-{key_name.lower().replace(\"_\", \"-\")}-here':\n",
    "            print(f\" {provider:<10} - Configured\")\n",
    "            configured_providers.append(provider)\n",
    "        else:\n",
    "            print(f\"üí§ {provider:<10} - Not configured (optional)\")\n",
    "    \n",
    "    # Smart configuration assessment\n",
    "    print(f\"\\n Setup Assessment:\")\n",
    "    if use_local:\n",
    "        print(\" Ready for FREE local development\")\n",
    "        print(\" You can start building immediately with zero costs\")\n",
    "        if configured_providers:\n",
    "            print(f\"üìà Plus {len(configured_providers)} cloud provider(s) for scaling\")\n",
    "        print(\" Llama 3.1 provides enterprise-grade capabilities for agent development\")\n",
    "    elif configured_providers:\n",
    "        print(f\" Ready for cloud development with {len(configured_providers)} provider(s)\")\n",
    "    else:\n",
    "        print(\" No providers configured\")\n",
    "        print(\" Recommendation: Keep USE_LOCAL_MODELS=true for FREE learning\")\n",
    "    \n",
    "    print(\"\\n Next: Test Google ADK framework\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Configuration error: {e}\")\n",
    "    print(\"\\n TROUBLESHOOTING:\")\n",
    "    print(\"1. Make sure .env file exists (copy from .env.example)\")\n",
    "    print(\"2. Check file permissions\")\n",
    "    print(\"3. Verify environment variable syntax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6117a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GOOGLE ADK FRAMEWORK TEST\n",
      "=============================================\n",
      " Google ADK imported successfully\n",
      " Testing with local model: ollama/llama3.1:1b\n",
      " Cost: $0.00 (FREE local development)\n",
      " Optimized for: Coding, agents, enterprise tasks\n",
      " Google ADK Agent created successfully\n",
      " Agent name: WorkspaceVerificationAgent\n",
      " Model: ollama/llama3.1:1b\n",
      " Instruction configured: ‚úÖ\n",
      "\n",
      "üîç ADK Framework Capabilities:\n",
      " Agent creation and configuration\n",
      " Model abstraction layer\n",
      " Instruction-based behavior\n",
      " Enterprise-grade architecture\n",
      " Multi-provider model support\n",
      "\n",
      " WORKSPACE SETUP COMPLETE!\n",
      " Your Google ADK development workspace is fully ready!\n",
      " Llama 3.1 optimized for coding and agent tasks\n",
      "\n",
      "üí° Pro Tips:\n",
      " You're starting with FREE Llama 3.1 models - perfect for learning coding and agents!\n",
      " Add cloud API keys to .env when you want to scale\n",
      " Local development = unlimited experimentation with zero cost\n",
      " Llama 3.1 is optimized for the exact use cases you'll build in this course\n"
     ]
    }
   ],
   "source": [
    "# Google ADK Framework Test\n",
    "print(\" GOOGLE ADK FRAMEWORK TEST\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "try:\n",
    "    # Import Google ADK\n",
    "    from google.adk.agents import Agent\n",
    "    print(\" Google ADK imported successfully\")\n",
    "    \n",
    "    # Load configuration\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Determine which model to use\n",
    "    use_local = os.getenv('USE_LOCAL_MODELS', 'true').lower() == 'true'\n",
    "    preferred_provider = os.getenv('PREFERRED_PROVIDER', 'local')\n",
    "    \n",
    "    if use_local and preferred_provider == 'local':\n",
    "        # Test with local model configuration\n",
    "        model_name = \"ollama/llama3.1:1b\"  # ADK format for Ollama\n",
    "        print(f\" Testing with local model: {model_name}\")\n",
    "        print(\" Cost: $0.00 (FREE local development)\")\n",
    "        print(\" Optimized for: Coding, agents, enterprise tasks\")\n",
    "    else:\n",
    "        # Test with cloud model\n",
    "        model_name = os.getenv('GOOGLE_MODEL', 'gemini-1.5-flash')\n",
    "        print(f\" Testing with cloud model: {model_name}\")\n",
    "        print(\" Cost: ~$0.075 per 1M tokens\")\n",
    "    \n",
    "    # Create test agent (framework validation)\n",
    "    test_agent = Agent(\n",
    "        name=\"WorkspaceVerificationAgent\",\n",
    "        model=model_name,\n",
    "        instruction=\"You are a test agent created to verify the Google ADK development workspace is properly configured. You excel at coding tasks and enterprise agent workflows.\"\n",
    "    )\n",
    "    \n",
    "    print(\" Google ADK Agent created successfully\")\n",
    "    print(f\" Agent name: {test_agent.name}\")\n",
    "    print(f\" Model: {test_agent.model}\")\n",
    "    print(f\" Instruction configured: {'‚úÖ' if test_agent.instruction else '‚ùå'}\")\n",
    "    \n",
    "    # Framework capabilities check\n",
    "    print(\"\\nüîç ADK Framework Capabilities:\")\n",
    "    print(\" Agent creation and configuration\")\n",
    "    print(\" Model abstraction layer\")\n",
    "    print(\" Instruction-based behavior\")\n",
    "    print(\" Enterprise-grade architecture\")\n",
    "    print(\" Multi-provider model support\")\n",
    "    \n",
    "    print(\"\\n WORKSPACE SETUP COMPLETE!\")\n",
    "    print(\" Your Google ADK development workspace is fully ready!\")\n",
    "    \n",
    "    if use_local:\n",
    "        print(\" Llama 3.1 optimized for coding and agent tasks\")\n",
    "    \n",
    "    print(\"\\nüí° Pro Tips:\")\n",
    "    if use_local:\n",
    "        print(\" You're starting with FREE Llama 3.1 models - perfect for learning coding and agents!\")\n",
    "        print(\" Add cloud API keys to .env when you want to scale\")\n",
    "        print(\" Local development = unlimited experimentation with zero cost\")\n",
    "        print(\" Llama 3.1 is optimized for the exact use cases you'll build in this course\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\" Google ADK import failed: {e}\")\n",
    "    print(\"\\n TROUBLESHOOTING:\")\n",
    "    print(\"1. Ensure virtual environment is activated\")\n",
    "    print(\"2. Run: pip install -r requirements.txt\")\n",
    "    print(\"3. Restart Jupyter kernel\")\n",
    "    print(\"4. Check requirements.txt includes google-adk\")\n",
    "    print(\"5. Verify kernel is set to 'Google ADK Multi-Provider'\")\n",
    "except Exception as e:\n",
    "    print(f\" Setup issue: {e}\")\n",
    "    print(\" Check your .env configuration\")\n",
    "    print(\" Verify Ollama is running if using local models\")\n",
    "    print(\" Ensure all dependencies are properly installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02cf52d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
