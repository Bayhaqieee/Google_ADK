{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc460c5",
   "metadata": {},
   "source": [
    "## Manual Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e4c4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Task\\Google-ADK\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENT MEMORY & CONVERSATION MANAGEMENT\n",
      "=============================================\n",
      "Session: 2025-07-27 22:02:52\n",
      "Focus: Vector embeddings and similarity search\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "d:\\Coding Task\\Google-ADK\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\adity\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Vector Embeddings:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embedding for: 'Lingkang kuli kuli wachang lingkangku lingkangku' (dimension: 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 17.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embedding for: 'Hidup Blonde!' (dimension: 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 24.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embedding for: 'Yo ndak tau kok tanya saya' (dimension: 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 21.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embedding for: 'Kamu siapa? Aku siapa?' (dimension: 384)\n",
      "\n",
      "Similarity Analysis:\n",
      "   'Lingkang kuli kuli wachang lin...' vs 'Hidup Blonde!...': 0.167\n",
      "   'Lingkang kuli kuli wachang lin...' vs 'Yo ndak tau kok tanya saya...': 0.348\n",
      "   'Lingkang kuli kuli wachang lin...' vs 'Kamu siapa? Aku siapa?...': 0.429\n",
      "   'Hidup Blonde!...' vs 'Yo ndak tau kok tanya saya...': 0.289\n",
      "   'Hidup Blonde!...' vs 'Kamu siapa? Aku siapa?...': 0.200\n",
      "   'Yo ndak tau kok tanya saya...' vs 'Kamu siapa? Aku siapa?...': 0.342\n",
      "\n",
      "Vector embedding system working:\n",
      "Sentence transformer model loaded\n",
      "Embedding dimension: {len(embeddings[0][1])}\n",
      "Cosine similarity calculation ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Vector Embeddings & Similarity Foundation\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"AGENT MEMORY & CONVERSATION MANAGEMENT\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Session: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"Focus: Vector embeddings and similarity search\")\n",
    "print()\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def create_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Create normalized embedding for text\"\"\"\n",
    "    embedding = embedding_model.encode([text])[0]\n",
    "    # Normalize for cosine similarity\n",
    "    return embedding / np.linalg.norm(embedding)\n",
    "\n",
    "def calculate_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
    "    \"\"\"Calculate cosine similarity between embeddings\"\"\"\n",
    "    return float(np.dot(embedding1, embedding2))\n",
    "\n",
    "# Test embedding functionality\n",
    "print(\"üîç Testing Vector Embeddings:\")\n",
    "test_texts = [\n",
    "    \"Lingkang kuli kuli wachang lingkangku lingkangku\",\n",
    "    \"Hidup Blonde!\", \n",
    "    \"Yo ndak tau kok tanya saya\",\n",
    "    \"Kamu siapa? Aku siapa?\"\n",
    "]\n",
    "\n",
    "embeddings = []\n",
    "for text in test_texts:\n",
    "    embedding = create_embedding(text)\n",
    "    embeddings.append((text, embedding))\n",
    "    print(f\"Created embedding for: '{text}' (dimension: {len(embedding)})\")\n",
    "\n",
    "# Test similarity\n",
    "print(\"\\nSimilarity Analysis:\")\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i+1, len(embeddings)):\n",
    "        text1, emb1 = embeddings[i]\n",
    "        text2, emb2 = embeddings[j]\n",
    "        similarity = calculate_similarity(emb1, emb2)\n",
    "        print(f\"   '{text1[:30]}...' vs '{text2[:30]}...': {similarity:.3f}\")\n",
    "\n",
    "print(\"\\nVector embedding system working:\")\n",
    "print(\"Sentence transformer model loaded\")\n",
    "print(\"Embedding dimension: {len(embeddings[0][1])}\")\n",
    "print(\"Cosine similarity calculation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3c2dd",
   "metadata": {},
   "source": [
    "## Data Ingestion using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f44fad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Testing FAISS Vector Memory:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.21it/s]\n",
      "INFO:__main__:Added memory: 00ae0d3f - 'Python is a programming language known for simplic...'\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.40it/s]\n",
      "INFO:__main__:Added memory: 5f97ecba - 'Machine learning enables computers to learn from d...'\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.55it/s]\n",
      "INFO:__main__:Added memory: fcd91dfe - 'FAISS provides efficient similarity search for lar...'\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.27it/s]\n",
      "INFO:__main__:Added memory: 31c1f43d - 'Vector databases store high-dimensional embeddings...'\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11.92it/s]\n",
      "INFO:__main__:Added memory: 70e2f912 - 'Natural language processing helps computers unders...'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Vector Search:\n",
      "\n",
      "Query: 'programming languages'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 29.16it/s]\n",
      "INFO:__main__:Memory search for 'programming languages...' found 2 matches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match (score: 0.579): Python is a programming language known for simplicity and readability\n",
      "Match (score: 0.401): Natural language processing helps computers understand human language\n",
      "\n",
      "Query: 'artificial intelligence and learning'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.63it/s]\n",
      "INFO:__main__:Memory search for 'artificial intelligence and le...' found 2 matches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match (score: 0.522): Machine learning enables computers to learn from data automatically\n",
      "Match (score: 0.359): Natural language processing helps computers understand human language\n",
      "\n",
      "Query: 'search algorithms'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 38.03it/s]\n",
      "INFO:__main__:Memory search for 'search algorithms...' found 2 matches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match (score: 0.457): FAISS provides efficient similarity search for large vector collections\n",
      "Match (score: 0.401): Vector databases store high-dimensional embeddings for semantic search\n",
      "\n",
      "FAISS vector memory system ready:\n",
      "Total entries: {stats['total_entries']}\n",
      "Vector dimension: {stats['dimension']}\n",
      "FAISS index size: {stats['index_size']}\n"
     ]
    }
   ],
   "source": [
    "# FAISS Integration for Vector Search\n",
    "import faiss\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class MemoryEntry:\n",
    "    \"\"\"Structured memory entry with metadata\"\"\"\n",
    "    entry_id: str\n",
    "    content: str\n",
    "    embedding: np.ndarray\n",
    "    metadata: Dict[str, Any]\n",
    "    created_at: str\n",
    "\n",
    "class VectorMemorySystem:\n",
    "    \"\"\"Production vector memory with FAISS\"\"\"\n",
    "    \n",
    "    def __init__(self, dimension: int = 384):\n",
    "        self.dimension = dimension\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "        self.entries = []\n",
    "        self.entry_map = {}  # Maps FAISS index to entry_id\n",
    "        \n",
    "    def add_memory(self, content: str, metadata: Dict[str, Any] = None) -> str:\n",
    "        \"\"\"Add content to vector memory\"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "            \n",
    "        entry_id = str(uuid.uuid4())\n",
    "        embedding = create_embedding(content)\n",
    "        \n",
    "        # Create memory entry\n",
    "        entry = MemoryEntry(\n",
    "            entry_id=entry_id,\n",
    "            content=content,\n",
    "            embedding=embedding,\n",
    "            metadata=metadata,\n",
    "            created_at=datetime.now().isoformat()\n",
    "        )\n",
    "        \n",
    "        # Add to FAISS index\n",
    "        faiss_index = len(self.entries)\n",
    "        self.index.add(embedding.reshape(1, -1))\n",
    "        self.entries.append(entry)\n",
    "        self.entry_map[faiss_index] = entry_id\n",
    "        \n",
    "        logger.info(f\"Added memory: {entry_id[:8]} - '{content[:50]}...'\")\n",
    "        return entry_id\n",
    "    \n",
    "    def search_memory(self, query: str, top_k: int = 5, threshold: float = 0.3) -> List[Tuple[MemoryEntry, float]]:\n",
    "        \"\"\"Search memory using vector similarity\"\"\"\n",
    "        if len(self.entries) == 0:\n",
    "            return []\n",
    "        \n",
    "        query_embedding = create_embedding(query)\n",
    "        scores, indices = self.index.search(query_embedding.reshape(1, -1), min(top_k, len(self.entries)))\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if score >= threshold:\n",
    "                entry = self.entries[idx]\n",
    "                results.append((entry, float(score)))\n",
    "        \n",
    "        logger.info(f\"Memory search for '{query[:30]}...' found {len(results)} matches\")\n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get memory statistics\"\"\"\n",
    "        return {\n",
    "            \"total_entries\": len(self.entries),\n",
    "            \"index_size\": self.index.ntotal,\n",
    "            \"dimension\": self.dimension\n",
    "        }\n",
    "\n",
    "# Initialize vector memory\n",
    "vector_memory = VectorMemorySystem()\n",
    "\n",
    "# Test with knowledge entries\n",
    "print(\"\\nüîß Testing FAISS Vector Memory:\")\n",
    "knowledge_base = [\n",
    "    (\"Python is a programming language known for simplicity and readability\", {\"topic\": \"programming\"}),\n",
    "    (\"Machine learning enables computers to learn from data automatically\", {\"topic\": \"ai\"}),\n",
    "    (\"FAISS provides efficient similarity search for large vector collections\", {\"topic\": \"search\"}),\n",
    "    (\"Vector databases store high-dimensional embeddings for semantic search\", {\"topic\": \"databases\"}),\n",
    "    (\"Natural language processing helps computers understand human language\", {\"topic\": \"nlp\"})\n",
    "]\n",
    "\n",
    "for content, metadata in knowledge_base:\n",
    "    entry_id = vector_memory.add_memory(content, metadata)\n",
    "\n",
    "# Test search functionality\n",
    "print(\"\\nTesting Vector Search:\")\n",
    "test_queries = [\n",
    "    \"programming languages\",\n",
    "    \"artificial intelligence and learning\", \n",
    "    \"search algorithms\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    results = vector_memory.search_memory(query, top_k=2)\n",
    "    for entry, score in results:\n",
    "        print(f\"Match (score: {score:.3f}): {entry.content}\")\n",
    "\n",
    "print(\"\\nFAISS vector memory system ready:\")\n",
    "stats = vector_memory.get_stats()\n",
    "print(\"Total entries: {stats['total_entries']}\")\n",
    "print(\"Vector dimension: {stats['dimension']}\")\n",
    "print(\"FAISS index size: {stats['index_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c4d59",
   "metadata": {},
   "source": [
    "## Memory Conversation Management using SQlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef77fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Database initialized: agent_memory.db\n",
      "INFO:__main__:Created session: f884f9e9 for user demo_user\n",
      "INFO:__main__:Saved turn: 26501cf3 for session f884f9e9\n",
      "INFO:__main__:Saved turn: eef9b93b for session f884f9e9\n",
      "INFO:__main__:Saved turn: 850d8eac for session f884f9e9\n",
      "INFO:__main__:Retrieved 3 turns for session f884f9e9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Conversation Persistence:\n",
      "\n",
      "Testing History Retrieval:\n",
      "   Turn 1:\n",
      "     User: Hello, I'm learning about AI\n",
      "     Agent: Hi! I'd be happy to help you learn about AI. What specifically interests you?\n",
      "   Turn 2:\n",
      "     User: What is machine learning?\n",
      "     Agent: Machine learning is a subset of AI that enables computers to learn from data without explicit programming.\n",
      "   Turn 3:\n",
      "     User: How does it relate to neural networks?\n",
      "     Agent: Neural networks are one approach to machine learning, inspired by how biological brains process information.\n",
      "\n",
      "Conversation persistence ready:\n",
      "SQLite database: {conversation_manager.db_path}\n",
      "Session tracking with turn history\n",
      "Knowledge usage logging\n"
     ]
    }
   ],
   "source": [
    "# Conversation Persistence Manager\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ConversationTurn:\n",
    "    \"\"\"Individual conversation turn with metadata\"\"\"\n",
    "    turn_id: str\n",
    "    session_id: str\n",
    "    timestamp: str\n",
    "    user_message: str\n",
    "    agent_response: str\n",
    "    knowledge_used: List[str]\n",
    "\n",
    "class ConversationManager:\n",
    "    \"\"\"Manages conversation persistence with SQLite\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"agent_memory.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "        \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Sessions table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS sessions (\n",
    "                session_id TEXT PRIMARY KEY,\n",
    "                user_id TEXT NOT NULL,\n",
    "                started_at TEXT NOT NULL,\n",
    "                last_active TEXT NOT NULL,\n",
    "                total_turns INTEGER DEFAULT 0\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Conversation turns table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS turns (\n",
    "                turn_id TEXT PRIMARY KEY,\n",
    "                session_id TEXT NOT NULL,\n",
    "                timestamp TEXT NOT NULL,\n",
    "                user_message TEXT NOT NULL,\n",
    "                agent_response TEXT NOT NULL,\n",
    "                knowledge_used TEXT,\n",
    "                FOREIGN KEY (session_id) REFERENCES sessions (session_id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        logger.info(f\"Database initialized: {self.db_path}\")\n",
    "    \n",
    "    def create_session(self, user_id: str) -> str:\n",
    "        \"\"\"Create new conversation session\"\"\"\n",
    "        session_id = str(uuid.uuid4())\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT INTO sessions (session_id, user_id, started_at, last_active, total_turns)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "        ''', (session_id, user_id, timestamp, timestamp, 0))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        logger.info(f\"Created session: {session_id[:8]} for user {user_id}\")\n",
    "        return session_id\n",
    "    \n",
    "    def save_turn(self, session_id: str, user_message: str, agent_response: str, \n",
    "                  knowledge_used: List[str] = None) -> str:\n",
    "        \"\"\"Save conversation turn\"\"\"\n",
    "        turn_id = str(uuid.uuid4())\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        if knowledge_used is None:\n",
    "            knowledge_used = []\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Save turn\n",
    "        cursor.execute('''\n",
    "            INSERT INTO turns (turn_id, session_id, timestamp, user_message, agent_response, knowledge_used)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        ''', (turn_id, session_id, timestamp, user_message, agent_response, json.dumps(knowledge_used)))\n",
    "        \n",
    "        # Update session\n",
    "        cursor.execute('''\n",
    "            UPDATE sessions SET last_active = ?, total_turns = total_turns + 1\n",
    "            WHERE session_id = ?\n",
    "        ''', (timestamp, session_id))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        logger.info(f\"Saved turn: {turn_id[:8]} for session {session_id[:8]}\")\n",
    "        return turn_id\n",
    "    \n",
    "    def get_conversation_history(self, session_id: str, last_n: int = 5) -> List[ConversationTurn]:\n",
    "        \"\"\"Get recent conversation history\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            SELECT turn_id, session_id, timestamp, user_message, agent_response, knowledge_used\n",
    "            FROM turns WHERE session_id = ?\n",
    "            ORDER BY timestamp DESC LIMIT ?\n",
    "        ''', (session_id, last_n))\n",
    "        \n",
    "        turns = []\n",
    "        for row in cursor.fetchall():\n",
    "            turn = ConversationTurn(\n",
    "                turn_id=row[0],\n",
    "                session_id=row[1], \n",
    "                timestamp=row[2],\n",
    "                user_message=row[3],\n",
    "                agent_response=row[4],\n",
    "                knowledge_used=json.loads(row[5]) if row[5] else []\n",
    "            )\n",
    "            turns.append(turn)\n",
    "        \n",
    "        conn.close()\n",
    "        turns.reverse()  # Chronological order\n",
    "        \n",
    "        logger.info(f\"Retrieved {len(turns)} turns for session {session_id[:8]}\")\n",
    "        return turns\n",
    "\n",
    "# Initialize conversation manager\n",
    "conversation_manager = ConversationManager()\n",
    "\n",
    "# Test conversation persistence\n",
    "print(\"\\nTesting Conversation Persistence:\")\n",
    "test_session = conversation_manager.create_session(\"demo_user\")\n",
    "\n",
    "# Save some test conversations\n",
    "test_conversations = [\n",
    "    (\"Hello, I'm learning about AI\", \"Hi! I'd be happy to help you learn about AI. What specifically interests you?\"),\n",
    "    (\"What is machine learning?\", \"Machine learning is a subset of AI that enables computers to learn from data without explicit programming.\"),\n",
    "    (\"How does it relate to neural networks?\", \"Neural networks are one approach to machine learning, inspired by how biological brains process information.\")\n",
    "]\n",
    "\n",
    "for user_msg, agent_resp in test_conversations:\n",
    "    turn_id = conversation_manager.save_turn(test_session, user_msg, agent_resp, [\"ai_knowledge\"])\n",
    "\n",
    "# Test retrieval\n",
    "print(\"\\nTesting History Retrieval:\")\n",
    "history = conversation_manager.get_conversation_history(test_session, last_n=3)\n",
    "for i, turn in enumerate(history, 1):\n",
    "    print(f\"   Turn {i}:\")\n",
    "    print(f\"     User: {turn.user_message}\")\n",
    "    print(f\"     Agent: {turn.agent_response}\")\n",
    "\n",
    "print(\"\\nConversation persistence ready:\")\n",
    "print(\"SQLite database: {conversation_manager.db_path}\")\n",
    "print(\"Session tracking with turn history\")\n",
    "print(\"Knowledge usage logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a36953",
   "metadata": {},
   "source": [
    "## Agent Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bfbeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for gemini-.* from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for model-optimizer-.* from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for projects\\/.+\\/locations\\/.+\\/endpoints\\/.+ from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for projects\\/.+\\/locations\\/.+\\/publishers\\/google\\/models\\/gemini.+ from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for gemini-.* from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for model-optimizer-.* from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for projects\\/.+\\/locations\\/.+\\/endpoints\\/.+ from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for projects\\/.+\\/locations\\/.+\\/publishers\\/google\\/models\\/gemini.+ from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for gemini-.* from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for model-optimizer-.* from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for projects\\/.+\\/locations\\/.+\\/endpoints\\/.+ from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:google_adk.google.adk.models.registry:Updating LLM class for projects\\/.+\\/locations\\/.+\\/publishers\\/google\\/models\\/gemini.+ from <class 'google.adk.models.google_llm.Gemini'> to <class 'google.adk.models.google_llm.Gemini'>\n",
      "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created session: defb9018 for user demo_user\n",
      "INFO:__main__:Memory agent ready - Conversation: defb9018, ADK: adk_defb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory-enhanced agent ready:\n",
      "Vector knowledge retrieval integrated\n",
      "Conversation history tracking\n",
      "Context-aware response generation\n",
      "Session management fixed\n"
     ]
    }
   ],
   "source": [
    "# Memory-Enhanced Agent with ADK (Fixed Session Management)\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "from google.genai import types\n",
    "\n",
    "class MemoryAgent:\n",
    "    \"\"\"Agent with vector memory and conversation persistence\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vector_memory = vector_memory\n",
    "        self.conversation_manager = conversation_manager\n",
    "        self.current_session = None\n",
    "        self.adk_session_id = None  # Separate ADK session ID\n",
    "        \n",
    "    async def setup(self, user_id: str = \"demo_user\"):\n",
    "        \"\"\"Initialize memory-enhanced agent\"\"\"\n",
    "        # Create conversation session\n",
    "        self.current_session = self.conversation_manager.create_session(user_id)\n",
    "        \n",
    "        # Create separate ADK session ID\n",
    "        self.adk_session_id = f\"adk_{self.current_session}\"\n",
    "        \n",
    "        # Setup ADK agent\n",
    "        model = LiteLlm(model=\"ollama_chat/llama3.1:8b\")\n",
    "        \n",
    "        self.agent = Agent(\n",
    "            name=\"MemoryAgent\",\n",
    "            model=model,\n",
    "            instruction=\"\"\"You are an intelligent agent with persistent memory capabilities.\n",
    "\n",
    "            You have access to:\n",
    "            - A knowledge base with relevant information\n",
    "            - Previous conversation history with this user\n",
    "            - Context from past interactions\n",
    "\n",
    "            When responding:\n",
    "            1. Use relevant knowledge from your memory when helpful\n",
    "            2. Reference previous conversations naturally\n",
    "            3. Build on past context to provide personalized responses\n",
    "            4. Be conversational and remember what you've discussed\n",
    "\n",
    "            Always be helpful and maintain conversation continuity.\"\"\"\n",
    "        )\n",
    "        \n",
    "        self.session_service = InMemorySessionService()\n",
    "        self.runner = Runner(\n",
    "            agent=self.agent,\n",
    "            app_name=\"memory_agent\", \n",
    "            session_service=self.session_service\n",
    "        )\n",
    "        \n",
    "        # Create ADK session with the correct ID\n",
    "        await self.session_service.create_session(\n",
    "            app_name=\"memory_agent\",\n",
    "            user_id=user_id,\n",
    "            session_id=self.adk_session_id\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Memory agent ready - Conversation: {self.current_session[:8]}, ADK: {self.adk_session_id[:8]}\")\n",
    "    \n",
    "    async def chat(self, user_message: str) -> str:\n",
    "        \"\"\"Chat with memory-enhanced responses\"\"\"\n",
    "        # Search relevant knowledge\n",
    "        knowledge_results = self.vector_memory.search_memory(user_message, top_k=3, threshold=0.2)\n",
    "        relevant_knowledge = [entry.content for entry, score in knowledge_results]\n",
    "        \n",
    "        # Get conversation history\n",
    "        history = self.conversation_manager.get_conversation_history(self.current_session, last_n=3)\n",
    "        \n",
    "        # Build context\n",
    "        context_parts = []\n",
    "        \n",
    "        if relevant_knowledge:\n",
    "            context_parts.append(\"Relevant Knowledge:\")\n",
    "            for knowledge in relevant_knowledge:\n",
    "                context_parts.append(f\"- {knowledge}\")\n",
    "        \n",
    "        if history:\n",
    "            context_parts.append(\"\\nRecent Conversation:\")\n",
    "            for turn in history:\n",
    "                context_parts.append(f\"User: {turn.user_message}\")\n",
    "                context_parts.append(f\"Assistant: {turn.agent_response}\")\n",
    "        \n",
    "        context_parts.append(f\"\\nCurrent User Message: {user_message}\")\n",
    "        \n",
    "        enhanced_message = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Send to agent using the correct ADK session ID\n",
    "        message = types.Content(role=\"user\", parts=[types.Part(text=enhanced_message)])\n",
    "        \n",
    "        response = \"\"\n",
    "        async for event in self.runner.run_async(\n",
    "            user_id=\"memory_user\",\n",
    "            session_id=self.adk_session_id,  # Use ADK session ID here\n",
    "            new_message=message\n",
    "        ):\n",
    "            if event.is_final_response():\n",
    "                response = event.content.parts[0].text\n",
    "                break\n",
    "        \n",
    "        # Save conversation using conversation session ID\n",
    "        knowledge_used = [f\"{entry.content[:50]}...\" for entry, _ in knowledge_results]\n",
    "        self.conversation_manager.save_turn(self.current_session, user_message, response, knowledge_used)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Initialize memory agent\n",
    "memory_agent = MemoryAgent()\n",
    "await memory_agent.setup(\"demo_user\")\n",
    "\n",
    "print(\"Memory-enhanced agent ready:\")\n",
    "print(\"Vector knowledge retrieval integrated\")\n",
    "print(\"Conversation history tracking\")\n",
    "print(\"Context-aware response generation\")\n",
    "print(\"Session management fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ced6d8",
   "metadata": {},
   "source": [
    "## Agent Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a211ce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMORY SYSTEM DEMONSTRATION\n",
      "===================================\n",
      "\n",
      " Memory-Enhanced Conversations:\n",
      "\n",
      "--- Turn 1 ---\n",
      "üë§ User: Hi, I'm interested in learning about Python programming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.91it/s]\n",
      "INFO:__main__:Memory search for 'Hi, I'm interested in learning...' found 2 matches\n",
      "INFO:__main__:Retrieved 0 turns for session defb9018\n",
      "\u001b[92m22:10:45 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= llama3.1:8b; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.1:8b; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Saved turn: db9654e8 for session defb9018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Hello! It's great to hear that you're interested in learning about Python programming. We've discussed languages before, but it seems we're starting fresh this time.\n",
      "\n",
      "Python is indeed a fantastic choice for beginners and experienced programmers alike due to its simplicity and readability. I'd be happy to help you get started with some beginner-friendly resources. Would you like me to suggest some tutorials or online courses that can help you learn the basics of Python?\n",
      "\n",
      "Also, what specifically are you hoping to do with Python programming? Are you interested in web development, data analysis, artificial intelligence, or something else? Knowing your goals will help me provide more targeted guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Turn 2 ---\n",
      "üë§ User: What makes Python good for machine learning?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.83it/s]\n",
      "INFO:__main__:Memory search for 'What makes Python good for mac...' found 3 matches\n",
      "INFO:__main__:Retrieved 1 turns for session defb9018\n",
      "\u001b[92m22:13:34 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= llama3.1:8b; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.1:8b; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Saved turn: ac0f5a8c for session defb9018\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You're looking to explore the intersection of Python and machine learning. That's a fascinating area!\n",
      "\n",
      "Python is an excellent choice for machine learning because it has several libraries that make it easy to implement algorithms, including scikit-learn and TensorFlow. Additionally, Python's simplicity and readability help when working with complex machine learning concepts.\n",
      "\n",
      "In fact, we discussed this topic briefly earlier, but I'd be happy to dive deeper now. One of the reasons Python is well-suited for machine learning is that it has a vast number of libraries and tools available, making it easy to implement various algorithms and techniques.\n",
      "\n",
      "If you're interested in getting started with machine learning using Python, I can recommend some resources that cover the basics of scikit-learn and TensorFlow. We could also discuss how natural language processing (NLP) comes into play when working with text-based data, which is a common application area for machine learning.\n",
      "\n",
      "How about we explore some real-world examples or case studies where Python has been used successfully in machine learning?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Turn 3 ---\n",
      "üë§ User: Can you explain how vector search works?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.07it/s]\n",
      "INFO:__main__:Memory search for 'Can you explain how vector sea...' found 3 matches\n",
      "INFO:__main__:Retrieved 2 turns for session defb9018\n",
      "\u001b[92m22:16:18 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= llama3.1:8b; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.1:8b; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Saved turn: a08637f4 for session defb9018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Vector search is an exciting topic that's gaining popularity, especially with the rise of natural language processing and machine learning. Vector search is a technique used to find similar vectors in high-dimensional spaces. It's particularly useful for semantic search applications.\n",
      "\n",
      "In essence, vector search databases like FAISS (mentioned earlier) store high-dimensional embeddings, which are numerical representations of text or other data. These embeddings capture the semantic meaning of the input data and enable efficient similarity searches.\n",
      "\n",
      "Here's a simplified example: imagine you're building a search engine that allows users to find documents related to a specific topic. You can use word embeddings like Word2Vec or BERT to represent each document as a vector in a high-dimensional space. Then, when a user queries the system with a keyword, you can use FAISS to find the most similar vectors (i.e., documents) in the database.\n",
      "\n",
      "FAISS provides an efficient way to perform similarity searches on large collections of vectors by using techniques like hierarchical k-means and IVF-SQ8. This allows for fast lookup times even when dealing with massive datasets.\n",
      "\n",
      "We discussed NLP earlier, but now we're bridging that knowledge with vector search techniques. Would you like me to explain more about how this works in practice or provide some code examples?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Turn 4 ---\n",
      "üë§ User: Earlier you mentioned Python - what did you say about its advantages?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.27it/s]\n",
      "INFO:__main__:Memory search for 'Earlier you mentioned Python -...' found 2 matches\n",
      "INFO:__main__:Retrieved 3 turns for session defb9018\n",
      "\u001b[92m22:20:48 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= llama3.1:8b; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.1:8b; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Saved turn: 8fa2e366 for session defb9018\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You'd like a recap on the benefits of using Python! Don't worry, I've got it saved in my memory. To refresh your memory (no pun intended), I said that Python is an excellent choice for beginners and experienced programmers alike due to its simplicity and readability.\n",
      "\n",
      "I mentioned earlier that this makes it easier to work with complex concepts, including machine learning. Its vast number of libraries and tools also make it a great language for implementing various algorithms and techniques.\n",
      "\n",
      "In other words, Python's simplicity, readability, and extensive library support make it an ideal choice for many applications, including machine learning and natural language processing.\n",
      "\n",
      "Would you like me to elaborate on any specific aspect or provide some examples that demonstrate these advantages?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Turn 5 ---\n",
      "üë§ User: How do vector databases help with the machine learning concepts we discussed?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.82it/s]\n",
      "INFO:__main__:Memory search for 'How do vector databases help w...' found 3 matches\n",
      "INFO:__main__:Retrieved 3 turns for session defb9018\n",
      "\u001b[92m22:25:58 - LiteLLM:INFO\u001b[0m: utils.py:3230 - \n",
      "LiteLLM completion() model= llama3.1:8b; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.1:8b; provider = ollama_chat\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Saved turn: ffcc40aa for session defb9018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You're connecting the dots between vector databases and machine learning. That's a great question!\n",
      "\n",
      "Vector databases, such as those using FAISS for efficient similarity search, can greatly enhance the machine learning workflow by providing an efficient way to store and query high-dimensional embeddings.\n",
      "\n",
      "Think of it this way: in machine learning, you often have large datasets with complex relationships between data points. Vector databases help you represent these relationships in a compact and computationally efficient manner. This enables fast lookup times for similar vectors, which is particularly useful when working with text-based data, such as natural language processing tasks.\n",
      "\n",
      "By integrating vector databases into your machine learning pipeline, you can:\n",
      "\n",
      "1. **Efficiently store** high-dimensional embeddings for large datasets.\n",
      "2. **Speed up similarity searches**, allowing you to quickly identify relevant patterns or relationships in the data.\n",
      "3. **Improve model performance**, by leveraging efficient storage and query mechanisms.\n",
      "\n",
      "For example, imagine using a vector database like FAISS to store word embeddings from a text corpus. You can then use this database to efficiently search for similar documents or topics, which can be a crucial step in many NLP tasks.\n",
      "\n",
      "Does that help clarify how vector databases relate to machine learning concepts?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEMORY SYSTEM ANALYSIS\n",
      "==============================\n",
      "\n",
      "Vector Memory:\n",
      "Knowledge Entries: 5\n",
      "Vector Dimension: 384\n",
      "FAISS Index Size: 5\n",
      "\n",
      "Knowledge Retrieval Test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.54it/s]\n",
      "INFO:__main__:Memory search for 'programming and artificial int...' found 3 matches\n",
      "INFO:__main__:Retrieved 5 turns for session defb9018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'programming and artificial intelligence'\n",
      "‚úÖ 0.468: Python is a programming language known for simplicity and re...\n",
      "‚úÖ 0.431: Machine learning enables computers to learn from data automa...\n",
      "‚úÖ 0.425: Natural language processing helps computers understand human...\n",
      "\n",
      "Conversation History:\n",
      "Total turns in session: 5\n",
      "Session ID: defb9018\n"
     ]
    }
   ],
   "source": [
    "# Complete Memory System Demonstration\n",
    "async def chat(self, user_message: str) -> str:\n",
    "    \"\"\"Chat with memory-enhanced responses\"\"\"\n",
    "    # Search relevant knowledge\n",
    "    knowledge_results = self.vector_memory.search_memory(user_message, top_k=3, threshold=0.2)\n",
    "    relevant_knowledge = [entry.content for entry, score in knowledge_results]\n",
    "    \n",
    "    # Get conversation history\n",
    "    history = self.conversation_manager.get_conversation_history(self.current_session, last_n=3)\n",
    "    \n",
    "    # Build context\n",
    "    context_parts = []\n",
    "    \n",
    "    if relevant_knowledge:\n",
    "        context_parts.append(\"Relevant Knowledge:\")\n",
    "        for knowledge in relevant_knowledge:\n",
    "            context_parts.append(f\"- {knowledge}\")\n",
    "    \n",
    "    if history:\n",
    "        context_parts.append(\"\\nRecent Conversation:\")\n",
    "        for turn in history:\n",
    "            context_parts.append(f\"User: {turn.user_message}\")\n",
    "            context_parts.append(f\"Assistant: {turn.agent_response}\")\n",
    "    \n",
    "    context_parts.append(f\"\\nCurrent User Message: {user_message}\")\n",
    "    \n",
    "    enhanced_message = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # Send to agent using the correct user_id\n",
    "    message = types.Content(role=\"user\", parts=[types.Part(text=enhanced_message)])\n",
    "    \n",
    "    response = \"\"\n",
    "    async for event in self.runner.run_async(\n",
    "        user_id=\"demo_user\",  # Fixed: use same user_id as setup\n",
    "        session_id=self.adk_session_id,\n",
    "        new_message=message\n",
    "    ):\n",
    "        if event.is_final_response():\n",
    "            response = event.content.parts[0].text\n",
    "            break\n",
    "    \n",
    "    # Save conversation\n",
    "    knowledge_used = [f\"{entry.content[:50]}...\" for entry, _ in knowledge_results]\n",
    "    self.conversation_manager.save_turn(self.current_session, user_message, response, knowledge_used)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Update the method properly\n",
    "MemoryAgent.chat = chat\n",
    "\n",
    "async def demonstrate_memory_system():\n",
    "    \"\"\"Test memory-enhanced conversations\"\"\"\n",
    "    \n",
    "    print(\"MEMORY SYSTEM DEMONSTRATION\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    conversations = [\n",
    "        \"Hi, I'm interested in learning about Python programming\",\n",
    "        \"What makes Python good for machine learning?\", \n",
    "        \"Can you explain how vector search works?\",\n",
    "        \"Earlier you mentioned Python - what did you say about its advantages?\",\n",
    "        \"How do vector databases help with the machine learning concepts we discussed?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n Memory-Enhanced Conversations:\")\n",
    "    \n",
    "    for i, user_message in enumerate(conversations, 1):\n",
    "        print(f\"\\n--- Turn {i} ---\")\n",
    "        print(f\"üë§ User: {user_message}\")\n",
    "        \n",
    "        response = await memory_agent.chat(user_message)\n",
    "        print(f\"Agent: {response}\")\n",
    "        \n",
    "        await asyncio.sleep(0.5)\n",
    "\n",
    "# Run demonstration\n",
    "await demonstrate_memory_system()\n",
    "\n",
    "print(\"\\nMEMORY SYSTEM ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Memory statistics\n",
    "memory_stats = vector_memory.get_stats()\n",
    "print(\"\\nVector Memory:\")\n",
    "print(f\"Knowledge Entries: {memory_stats['total_entries']}\")\n",
    "print(f\"Vector Dimension: {memory_stats['dimension']}\")\n",
    "print(f\"FAISS Index Size: {memory_stats['index_size']}\")\n",
    "\n",
    "# Test knowledge retrieval\n",
    "print(\"\\nKnowledge Retrieval Test:\")\n",
    "test_query = \"programming and artificial intelligence\"\n",
    "results = vector_memory.search_memory(test_query, top_k=3)\n",
    "print(f\"Query: '{test_query}'\")\n",
    "for entry, score in results:\n",
    "    print(f\"‚úÖ {score:.3f}: {entry.content[:60]}...\")\n",
    "\n",
    "# Conversation history\n",
    "history = conversation_manager.get_conversation_history(memory_agent.current_session)\n",
    "print(\"\\nConversation History:\")\n",
    "print(f\"Total turns in session: {len(history)}\")\n",
    "print(f\"Session ID: {memory_agent.current_session[:8]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d9681",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
