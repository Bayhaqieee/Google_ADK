{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "13b63e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build 4 proven patterns that deliver consistent results\n",
      "Error initializing agent: \"Session\" object has no field \"session\"\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import uuid\n",
    "from google.adk import Agent\n",
    "from google.adk.events import Event\n",
    "from google.genai.types import Content, Part\n",
    "from google.adk.sessions import Session\n",
    "from typing import Dict, List\n",
    "\n",
    "print(\"Build 4 proven patterns that deliver consistent results\")\n",
    "\n",
    "try:\n",
    "    test_agent = Agent(name=\"tester\", model=\"llama3.2:3b\")\n",
    "    # Create a single session with required fields\n",
    "    adk_session = Session(\n",
    "        id=str(uuid.uuid4()),\n",
    "        appName=\"test-app\",\n",
    "        userId=\"test-user-123\"\n",
    "    )\n",
    "    # Manually attach the session to itself to satisfy the agent's internal context requirement\n",
    "    adk_session.session = adk_session\n",
    "    \n",
    "    print(\"Agent and its session object initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing agent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635312a7",
   "metadata": {},
   "source": [
    "## Effective Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c2cd9440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proven Patterns loaded!\n"
     ]
    }
   ],
   "source": [
    "class ProvenPromptPatterns:\n",
    "    @staticmethod\n",
    "    def chain_of_thought(problem: str, context: str = \"\") -> str:\n",
    "        return f\"\"\"\n",
    "PROBLEM: {problem}\n",
    "CONTEXT: {context}\n",
    "\n",
    "Think step-by-step:\n",
    "1. Break down the problem\n",
    "2. Analyze components\n",
    "3. Synthesize solution\n",
    "\n",
    "Provide reasoning for each step.\n",
    "\"\"\".strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def role_based(task: str, role: str, expertise: str = \"\") -> str:\n",
    "        return f\"\"\"\n",
    "You are an expert {role}.\n",
    "Expertise: {expertise}\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "Provide expert recommendations with:\n",
    "1. Professional best practices\n",
    "2. Industry standards\n",
    "3. Practical solutions\n",
    "\"\"\".strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def template_based(content: str, format_template: str) -> str:\n",
    "        return f\"\"\"\n",
    "CONTENT: {content}\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "{format_template}\n",
    "\n",
    "Follow the exact structure above.\n",
    "\"\"\".strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def step_by_step(goal: str, process: str) -> str:\n",
    "        return f\"\"\"\n",
    "GOAL: {goal}\n",
    "PROCESS: {process}\n",
    "\n",
    "Execute systematically:\n",
    "- State what you're doing\n",
    "- Show your work\n",
    "- Verify completion\n",
    "\n",
    "Provide detailed execution.\n",
    "\"\"\".strip()\n",
    "\n",
    "patterns = ProvenPromptPatterns()\n",
    "print(\"Proven Patterns loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f04ca",
   "metadata": {},
   "source": [
    "## Real Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "778b82c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent received. Running tasks by passing the full Session object...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Session' object has no attribute 'session'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAverage pattern score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_score*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Pass the agent, validator, and session into the main function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main(test_agent, validator, adk_session)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(agent, validator, session)\u001b[39m\n\u001b[32m      9\u001b[39m session.events.append(context_event)\n\u001b[32m     10\u001b[39m response_chunks = []\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m agent.run_async(session):\n\u001b[32m     12\u001b[39m     session.events.append(chunk)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(chunk, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(chunk.content, \u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding Task\\Google-ADK\\venv\\Lib\\site-packages\\google\\adk\\agents\\base_agent.py:204\u001b[39m, in \u001b[36mBaseAgent.run_async\u001b[39m\u001b[34m(self, parent_context)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33magent_run [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    202\u001b[39m   ctx = \u001b[38;5;28mself\u001b[39m._create_invocation_context(parent_context)\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m event := \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__handle_before_agent_callback(ctx):\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    206\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m ctx.end_invocation:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding Task\\Google-ADK\\venv\\Lib\\site-packages\\google\\adk\\agents\\base_agent.py:355\u001b[39m, in \u001b[36mBaseAgent.__handle_before_agent_callback\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__handle_before_agent_callback\u001b[39m(\n\u001b[32m    345\u001b[39m     \u001b[38;5;28mself\u001b[39m, ctx: InvocationContext\n\u001b[32m    346\u001b[39m ) -> Optional[Event]:\n\u001b[32m    347\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Runs the before_agent_callback if it exists.\u001b[39;00m\n\u001b[32m    348\u001b[39m \n\u001b[32m    349\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    353\u001b[39m \u001b[33;03m    Optional[Event]: an event if callback provides content or changed state.\u001b[39;00m\n\u001b[32m    354\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m   callback_context = \u001b[43mCallbackContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m   \u001b[38;5;66;03m# Run callbacks from the plugins.\u001b[39;00m\n\u001b[32m    358\u001b[39m   before_agent_callback_content = (\n\u001b[32m    359\u001b[39m       \u001b[38;5;28;01mawait\u001b[39;00m ctx.plugin_manager.run_before_agent_callback(\n\u001b[32m    360\u001b[39m           agent=\u001b[38;5;28mself\u001b[39m, callback_context=callback_context\n\u001b[32m    361\u001b[39m       )\n\u001b[32m    362\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding Task\\Google-ADK\\venv\\Lib\\site-packages\\google\\adk\\agents\\callback_context.py:52\u001b[39m, in \u001b[36mCallbackContext.__init__\u001b[39m\u001b[34m(self, invocation_context, event_actions)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# TODO(weisun): make this public for Agent Development Kit, but private for\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# users.\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mself\u001b[39m._event_actions = event_actions \u001b[38;5;129;01mor\u001b[39;00m EventActions()\n\u001b[32m     51\u001b[39m \u001b[38;5;28mself\u001b[39m._state = State(\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     value=\u001b[43minvocation_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m.state,\n\u001b[32m     53\u001b[39m     delta=\u001b[38;5;28mself\u001b[39m._event_actions.state_delta,\n\u001b[32m     54\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding Task\\Google-ADK\\venv\\Lib\\site-packages\\pydantic\\main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Session' object has no attribute 'session'"
     ]
    }
   ],
   "source": [
    "async def main(agent, validator, session):\n",
    "    print(\"Agent received. Running tasks by passing the full Session object...\")\n",
    "\n",
    "    # Test Chain-of-Thought\n",
    "    cot_prompt = patterns.chain_of_thought(\"Our customer churn rate has increased by 15% in the last quarter.\", \"We need to understand the root causes and devise a retention strategy.\")\n",
    "    agent.generate_content_config = {\"temperature\": 0.3}\n",
    "    prompt_content = Content(parts=[Part(text=cot_prompt)])\n",
    "    context_event = Event(content=prompt_content, author=\"user\")\n",
    "    session.events.append(context_event)\n",
    "    response_chunks = []\n",
    "    async for chunk in agent.run_async(session):\n",
    "        session.events.append(chunk)\n",
    "        if hasattr(chunk, 'content') and hasattr(chunk.content, 'text'):\n",
    "            response_chunks.append(chunk.content.text)\n",
    "    cot_response = \"\".join(response_chunks)\n",
    "    print(f\"Chain-of-Thought Response: {len(cot_response)} chars\")\n",
    "\n",
    "    # Test Role-Based\n",
    "    role_prompt = patterns.role_based(\"Recommend a scalable cloud architecture for a new social media app.\", \"Senior Cloud Architect\", \"AWS, Kubernetes, Microservices\")\n",
    "    agent.generate_content_config = {\"temperature\": 0.5}\n",
    "    prompt_content = Content(parts=[Part(text=role_prompt)])\n",
    "    context_event = Event(content=prompt_content, author=\"user\")\n",
    "    session.events.append(context_event)\n",
    "    response_chunks = []\n",
    "    async for chunk in agent.run_async(session):\n",
    "        session.events.append(chunk)\n",
    "        if hasattr(chunk, 'content') and hasattr(chunk.content, 'text'):\n",
    "            response_chunks.append(chunk.content.text)\n",
    "    role_response = \"\".join(response_chunks)\n",
    "    print(f\"Role-Based Response: {len(role_response)} chars\")\n",
    "\n",
    "    # Test Template-Based\n",
    "    template_prompt = patterns.template_based(\n",
    "        content=\"User-reported issues: Login failures, slow dashboard loading, data export errors.\",\n",
    "        format_template=\"\"\"Subject: [Summarize the Main Issue]\n",
    "Dear [User Name],\n",
    "We are addressing the issues you reported: [List of Issues].\n",
    "Our team will take these actions: [List of Actions].\n",
    "Update ETA: [Provide an ETA].\"\"\"\n",
    "    )\n",
    "    agent.generate_content_config = {\"temperature\": 0.1}\n",
    "    prompt_content = Content(parts=[Part(text=template_prompt)])\n",
    "    context_event = Event(content=prompt_content, author=\"user\")\n",
    "    session.events.append(context_event)\n",
    "    response_chunks = []\n",
    "    async for chunk in agent.run_async(session):\n",
    "        session.events.append(chunk)\n",
    "        if hasattr(chunk, 'content') and hasattr(chunk.content, 'text'):\n",
    "            response_chunks.append(chunk.content.text)\n",
    "    template_response = \"\".join(response_chunks)\n",
    "    print(f\"Template-Based Response: {len(template_response)} chars\")\n",
    "\n",
    "    # Test Step-by-Step\n",
    "    step_prompt = patterns.step_by_step(\n",
    "        goal=\"Contain and resolve a security breach.\",\n",
    "        process=\"Follow this incident response framework: 1. Containment 2. Threat Assessment 3. Investigation 4. Recovery 5. Post-Incident Communication\"\n",
    "    )\n",
    "    agent.generate_content_config = {\"temperature\": 0.2}\n",
    "    prompt_content = Content(parts=[Part(text=step_prompt)])\n",
    "    context_event = Event(content=prompt_content, author=\"user\")\n",
    "    session.events.append(context_event)\n",
    "    response_chunks = []\n",
    "    async for chunk in agent.run_async(session):\n",
    "        session.events.append(chunk)\n",
    "        if hasattr(chunk, 'content') and hasattr(chunk.content, 'text'):\n",
    "            response_chunks.append(chunk.content.text)\n",
    "    step_response = \"\".join(response_chunks)\n",
    "    print(f\"Step-by-Step Response: {len(step_response)} chars\")\n",
    "    \n",
    "    print(\"\\n--- Running Validation ---\")\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"Chain-of-Thought\", cot_response, [\"step\", \"analysis\", \"churn\"]),\n",
    "        (\"Role-Based\", role_response, [\"architecture\", \"scalable\", \"recommend\"]),\n",
    "        (\"Template-Based\", template_response, [\"subject\", \"issue\", \"actions\"]),\n",
    "        (\"Step-by-Step\", step_response, [\"contain\", \"assess\", \"recover\"])\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for name, response, expected in test_cases:\n",
    "        metrics = validator.validate_response(response, expected)\n",
    "        score = validator.calculate_score(metrics)\n",
    "        scores.append(score)\n",
    "        print(f\"{name}: {score:.2f} score\")\n",
    "\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    print(f\"\\nAverage pattern score: {avg_score:.2f} ({avg_score*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# Pass the agent, validator, and session into the main function\n",
    "await main(test_agent, validator, adk_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fadf2",
   "metadata": {},
   "source": [
    "## Automated Validation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f8a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python313\\Lib\\ast.py:54: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  return compile(source, filename, mode, flags,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cot_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     30\u001b[39m validator = PromptValidator()\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Validate all responses\u001b[39;00m\n\u001b[32m     33\u001b[39m test_cases = [\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mChain-of-Thought\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mcot_response\u001b[49m, [\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33manalysis\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mchurn\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     35\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mRole-Based\u001b[39m\u001b[33m\"\u001b[39m, role_response, [\u001b[33m\"\u001b[39m\u001b[33marchitecture\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscalable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrecommend\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     36\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mTemplate-Based\u001b[39m\u001b[33m\"\u001b[39m, template_response, [\u001b[33m\"\u001b[39m\u001b[33msubject\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33missue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mactions\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     37\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mStep-by-Step\u001b[39m\u001b[33m\"\u001b[39m, step_response, [\u001b[33m\"\u001b[39m\u001b[33mcontain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33massess\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrecover\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     38\u001b[39m ]\n\u001b[32m     40\u001b[39m scores = []\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, response, expected \u001b[38;5;129;01min\u001b[39;00m test_cases:\n",
      "\u001b[31mNameError\u001b[39m: name 'cot_response' is not defined"
     ]
    }
   ],
   "source": [
    "class PromptValidator:\n",
    "    def validate_response(self, response: str, expected: List[str] = None) -> Dict[str, float]:\n",
    "        metrics = {}\n",
    "        \n",
    "        # Completeness\n",
    "        metrics[\"completeness\"] = min(1.0, len(response) / 500) if len(response) > 200 else 0.3\n",
    "        \n",
    "        # Structure\n",
    "        structure_count = sum(1 for ind in [\":\", \"-\", \"1.\", \"\\n\"] if ind in response)\n",
    "        metrics[\"structure\"] = min(1.0, structure_count / 4)\n",
    "        \n",
    "        # Relevance\n",
    "        if expected:\n",
    "            found = sum(1 for elem in expected if elem.lower() in response.lower())\n",
    "            metrics[\"relevance\"] = found / len(expected)\n",
    "        else:\n",
    "            metrics[\"relevance\"] = 0.8\n",
    "        \n",
    "        # Actionability\n",
    "        action_words = [\"recommend\", \"suggest\", \"should\", \"action\", \"step\"]\n",
    "        action_count = sum(1 for word in action_words if word in response.lower())\n",
    "        metrics[\"actionability\"] = min(1.0, action_count / 3)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_score(self, metrics: Dict[str, float]) -> float:\n",
    "        weights = {\"completeness\": 0.2, \"relevance\": 0.4, \"structure\": 0.2, \"actionability\": 0.2}\n",
    "        return sum(metrics[key] * weights[key] for key in weights)\n",
    "\n",
    "validator = PromptValidator()\n",
    "\n",
    "# Validate all responses\n",
    "test_cases = [\n",
    "    (\"Chain-of-Thought\", cot_response, [\"step\", \"analysis\", \"churn\"]),\n",
    "    (\"Role-Based\", role_response, [\"architecture\", \"scalable\", \"recommend\"]),\n",
    "    (\"Template-Based\", template_response, [\"subject\", \"issue\", \"actions\"]),\n",
    "    (\"Step-by-Step\", step_response, [\"contain\", \"assess\", \"recover\"])\n",
    "]\n",
    "\n",
    "scores = []\n",
    "for name, response, expected in test_cases:\n",
    "    metrics = validator.validate_response(response, expected)\n",
    "    score = validator.calculate_score(metrics)\n",
    "    scores.append(score)\n",
    "    print(f\"{name}: {score:.2f} score\")\n",
    "\n",
    "avg_score = sum(scores) / len(scores)\n",
    "print(f\"\\nAverage pattern score: {avg_score:.2f} ({avg_score*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311aad0c",
   "metadata": {},
   "source": [
    "## A/B Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866bad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptABTester:\n",
    "    def __init__(self, validator):\n",
    "        self.validator = validator\n",
    "        self.results = []\n",
    "    \n",
    "    async def run_test(self, agent, prompt_a, prompt_b, test_name, expected=None):\n",
    "        response_a = await agent.run(prompt_a)\n",
    "        response_b = await agent.run(prompt_b)\n",
    "        \n",
    "        score_a = self.validator.calculate_score(self.validator.validate_response(response_a, expected))\n",
    "        score_b = self.validator.calculate_score(self.validator.validate_response(response_b, expected))\n",
    "        \n",
    "        improvement = ((score_b - score_a) / score_a) * 100 if score_a > 0 else 0\n",
    "        \n",
    "        result = {\n",
    "            \"test\": test_name,\n",
    "            \"score_a\": score_a,\n",
    "            \"score_b\": score_b,\n",
    "            \"improvement\": improvement\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        \n",
    "        print(f\"\\n{test_name}:\")\n",
    "        print(f\"A: {score_a:.3f}, B: {score_b:.3f}, Improvement: {improvement:+.1f}%\")\n",
    "        return result\n",
    "\n",
    "ab_tester = PromptABTester(validator)\n",
    "\n",
    "# Test 1: Basic vs Chain-of-Thought\n",
    "basic = \"Our app has 2.1 stars. Users complain about crashes. Create action plan.\"\n",
    "enhanced = patterns.chain_of_thought(\"App has 2.1 stars, crashes reported\", \"Mobile app performance issues\")\n",
    "\n",
    "await ab_tester.run_test(test_agent, basic, enhanced, \"Basic vs CoT\", [\"action\", \"plan\", \"crashes\"])\n",
    "\n",
    "# Test 2: Generic vs Role-Based  \n",
    "generic = \"Choose between React Native, Flutter, or native. Recommend best option.\"\n",
    "role_enhanced = patterns.role_based(\"Choose mobile framework\", \"Mobile Architect\", \"Cross-platform expertise\")\n",
    "\n",
    "await ab_tester.run_test(test_agent, generic, role_enhanced, \"Generic vs Role\", [\"react\", \"flutter\", \"recommend\"])\n",
    "\n",
    "# Calculate summary\n",
    "improvements = [r[\"improvement\"] for r in ab_tester.results]\n",
    "avg_improvement = sum(improvements) / len(improvements)\n",
    "print(f\"\\nAverage improvement: {avg_improvement:+.1f}%\")\n",
    "\n",
    "if avg_improvement >= 20:\n",
    "    print(\"Excellent: 20%+ consistent gains!\")\n",
    "else:\n",
    "    print(\"Good: Meaningful improvements shown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3952017",
   "metadata": {},
   "source": [
    "## Production Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbfeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionLibrary:\n",
    "    def __init__(self):\n",
    "        self.patterns = ProvenPromptPatterns()\n",
    "        self.usage = {\"cot\": 0, \"role\": 0, \"template\": 0, \"step\": 0}\n",
    "    \n",
    "    def get_best_pattern(self, task_type):\n",
    "        mapping = {\n",
    "            \"analysis\": \"cot\",\n",
    "            \"expert\": \"role\", \n",
    "            \"communication\": \"template\",\n",
    "            \"process\": \"step\"\n",
    "        }\n",
    "        return mapping.get(task_type, \"cot\")\n",
    "    \n",
    "    def create_prompt(self, task_type, **kwargs):\n",
    "        pattern = self.get_best_pattern(task_type)\n",
    "        self.usage[pattern] += 1\n",
    "        \n",
    "        if pattern == \"cot\":\n",
    "            return self.patterns.chain_of_thought(kwargs.get(\"problem\", \"\"), kwargs.get(\"context\", \"\"))\n",
    "        elif pattern == \"role\":\n",
    "            return self.patterns.role_based(kwargs.get(\"task\", \"\"), kwargs.get(\"role\", \"\"), kwargs.get(\"expertise\", \"\"))\n",
    "        elif pattern == \"template\":\n",
    "            return self.patterns.template_based(kwargs.get(\"content\", \"\"), kwargs.get(\"format\", \"\"))\n",
    "        elif pattern == \"step\":\n",
    "            return self.patterns.step_by_step(kwargs.get(\"goal\", \"\"), kwargs.get(\"process\", \"\"))\n",
    "\n",
    "library = ProductionLibrary()\n",
    "\n",
    "# Test automatic pattern selection\n",
    "scenarios = [\n",
    "    (\"analysis\", \"Market analysis task\"),\n",
    "    (\"expert\", \"Technical architecture\"), \n",
    "    (\"communication\", \"Customer email\"),\n",
    "    (\"process\", \"Security response\")\n",
    "]\n",
    "\n",
    "print(\"INTELLIGENT PATTERN SELECTION:\")\n",
    "for task_type, description in scenarios:\n",
    "    recommended = library.get_best_pattern(task_type)\n",
    "    print(f\"{description}: {recommended}\")\n",
    "\n",
    "# Generate some example prompts\n",
    "market_prompt = library.create_prompt(\n",
    "    \"analysis\", \n",
    "    problem=\"Competitor launched at 50% lower price\",\n",
    "    context=\"Enterprise B2B market\"\n",
    ")\n",
    "\n",
    "tech_prompt = library.create_prompt(\n",
    "    \"expert\",\n",
    "    task=\"Design rate limiting for 1M requests/hour\", \n",
    "    role=\"Backend Engineer\",\n",
    "    expertise=\"Microservices, Redis\"\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated market prompt: {len(market_prompt)} chars\")\n",
    "print(f\"Generated tech prompt: {len(tech_prompt)} chars\")\n",
    "print(f\"\\nUsage stats: {library.usage}\")\n",
    "print(\"Production library working perfectly!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
