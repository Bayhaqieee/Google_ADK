{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aa021fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build 4 proven patterns that deliver consistent results\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from google.adk import Agent\n",
    "from typing import Dict, List\n",
    "\n",
    "print(\"Build 4 proven patterns that deliver consistent results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13b63e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent and its patterns object initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent. The patterns object will be available through this instance.\n",
    "try:\n",
    "    test_agent = Agent(name=\"tester\", model=\"llama3.2:3b\")\n",
    "    print(\"Agent and its patterns object initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing agent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635312a7",
   "metadata": {},
   "source": [
    "## Effective Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2cd9440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proven Patterns loaded!\n"
     ]
    }
   ],
   "source": [
    "class ProvenPromptPatterns:\n",
    "    @staticmethod\n",
    "    def chain_of_thought(problem: str, context: str = \"\") -> str:\n",
    "        return f\"\"\"\n",
    "PROBLEM: {problem}\n",
    "CONTEXT: {context}\n",
    "\n",
    "Think step-by-step:\n",
    "1. Break down the problem\n",
    "2. Analyze components\n",
    "3. Synthesize solution\n",
    "\n",
    "Provide reasoning for each step.\n",
    "\"\"\".strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def role_based(task: str, role: str, expertise: str = \"\") -> str:\n",
    "        return f\"\"\"\n",
    "You are an expert {role}.\n",
    "Expertise: {expertise}\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "Provide expert recommendations with:\n",
    "1. Professional best practices\n",
    "2. Industry standards\n",
    "3. Practical solutions\n",
    "\"\"\".strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def template_based(content: str, format_template: str) -> str:\n",
    "        return f\"\"\"\n",
    "CONTENT: {content}\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "{format_template}\n",
    "\n",
    "Follow the exact structure above.\n",
    "\"\"\".strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def step_by_step(goal: str, process: str) -> str:\n",
    "        return f\"\"\"\n",
    "GOAL: {goal}\n",
    "PROCESS: {process}\n",
    "\n",
    "Execute systematically:\n",
    "- State what you're doing\n",
    "- Show your work\n",
    "- Verify completion\n",
    "\n",
    "Provide detailed execution.\n",
    "\"\"\".strip()\n",
    "\n",
    "patterns = ProvenPromptPatterns()\n",
    "print(\"Proven Patterns loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f04ca",
   "metadata": {},
   "source": [
    "## Real Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "778b82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(agent):\n",
    "    print(\"Agent received. Running tasks with the correct method...\")\n",
    "\n",
    "    # Test Chain-of-Thought\n",
    "    cot_prompt = \"Let's analyze the following situation step by step...\" # (Keeping prompts short for clarity)\n",
    "    cot_response = await agent.METHOD_NAME_HERE(cot_prompt, temperature=0.3)\n",
    "    print(f\"Chain-of-Thought Response: {len(cot_response)} chars\")\n",
    "\n",
    "    # Test Role-Based\n",
    "    role_prompt = \"Act as a Senior Cloud Architect...\"\n",
    "    role_response = await agent.METHOD_NAME_HERE(role_prompt, temperature=0.5)\n",
    "    print(f\"Role-Based Response: {len(role_response)} chars\")\n",
    "\n",
    "    # Test Template-Based by creating the prompt string manually\n",
    "    template_prompt = f\"\"\"\n",
    "        Fill out the following template based on these user-reported issues: \"Login issues, slow loads, missing data\".\n",
    "\n",
    "        Template:\n",
    "        Subject: [Summarize the Main Issue]\n",
    "        Dear [User Name],\n",
    "        We are writing to address the issues you reported: [List of Issues].\n",
    "        Our team will take the following actions: [List of Actions].\n",
    "        We expect to provide you with an update by: [Provide an ETA].\n",
    "        \"\"\"\n",
    "    template_response = await agent.run(template_prompt, temperature=0.1)\n",
    "    print(f\"Template-Based Response: {len(template_response)} chars\")\n",
    "\n",
    "    # Test Step-by-Step by creating the prompt string manually\n",
    "    step_prompt = f\"\"\"\n",
    "        Provide a plan to contain and resolve a security breach.\n",
    "        Follow this specific incident response framework:\n",
    "        1. Containment\n",
    "        2. Threat Assessment\n",
    "        3. Investigation\n",
    "        4. Recovery\n",
    "        5. Post-Incident Communication\n",
    "        \"\"\"\n",
    "    step_response = await agent.run(step_prompt, temperature=0.2)\n",
    "    print(f\"Step-by-Step Response: {len(step_response)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e0579c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent received. Running tasks with the correct method...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlmAgent' object has no attribute 'METHOD_NAME_HERE'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Pass the created test_agent into the main function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main(test_agent)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(agent)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Test Chain-of-Thought\u001b[39;00m\n\u001b[32m      5\u001b[39m cot_prompt = \u001b[33m\"\u001b[39m\u001b[33mLet\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms analyze the following situation step by step...\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# (Keeping prompts short for clarity)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m cot_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMETHOD_NAME_HERE\u001b[49m(cot_prompt, temperature=\u001b[32m0.3\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChain-of-Thought Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cot_response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chars\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Test Role-Based\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding Task\\Google-ADK\\venv\\Lib\\site-packages\\pydantic\\main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'LlmAgent' object has no attribute 'METHOD_NAME_HERE'"
     ]
    }
   ],
   "source": [
    "# Pass the created test_agent into the main function\n",
    "await main(test_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fadf2",
   "metadata": {},
   "source": [
    "## Automated Validation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b9f8a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python313\\Lib\\ast.py:54: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  return compile(source, filename, mode, flags,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cot_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     30\u001b[39m validator = PromptValidator()\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Validate all responses\u001b[39;00m\n\u001b[32m     33\u001b[39m test_cases = [\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mChain-of-Thought\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mcot_response\u001b[49m, [\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33manalysis\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mchurn\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     35\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mRole-Based\u001b[39m\u001b[33m\"\u001b[39m, role_response, [\u001b[33m\"\u001b[39m\u001b[33marchitecture\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscalable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrecommend\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     36\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mTemplate-Based\u001b[39m\u001b[33m\"\u001b[39m, template_response, [\u001b[33m\"\u001b[39m\u001b[33msubject\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33missue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mactions\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     37\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mStep-by-Step\u001b[39m\u001b[33m\"\u001b[39m, step_response, [\u001b[33m\"\u001b[39m\u001b[33mcontain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33massess\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrecover\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     38\u001b[39m ]\n\u001b[32m     40\u001b[39m scores = []\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, response, expected \u001b[38;5;129;01min\u001b[39;00m test_cases:\n",
      "\u001b[31mNameError\u001b[39m: name 'cot_response' is not defined"
     ]
    }
   ],
   "source": [
    "class PromptValidator:\n",
    "    def validate_response(self, response: str, expected: List[str] = None) -> Dict[str, float]:\n",
    "        metrics = {}\n",
    "        \n",
    "        # Completeness\n",
    "        metrics[\"completeness\"] = min(1.0, len(response) / 500) if len(response) > 200 else 0.3\n",
    "        \n",
    "        # Structure\n",
    "        structure_count = sum(1 for ind in [\":\", \"-\", \"1.\", \"\\n\"] if ind in response)\n",
    "        metrics[\"structure\"] = min(1.0, structure_count / 4)\n",
    "        \n",
    "        # Relevance\n",
    "        if expected:\n",
    "            found = sum(1 for elem in expected if elem.lower() in response.lower())\n",
    "            metrics[\"relevance\"] = found / len(expected)\n",
    "        else:\n",
    "            metrics[\"relevance\"] = 0.8\n",
    "        \n",
    "        # Actionability\n",
    "        action_words = [\"recommend\", \"suggest\", \"should\", \"action\", \"step\"]\n",
    "        action_count = sum(1 for word in action_words if word in response.lower())\n",
    "        metrics[\"actionability\"] = min(1.0, action_count / 3)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_score(self, metrics: Dict[str, float]) -> float:\n",
    "        weights = {\"completeness\": 0.2, \"relevance\": 0.4, \"structure\": 0.2, \"actionability\": 0.2}\n",
    "        return sum(metrics[key] * weights[key] for key in weights)\n",
    "\n",
    "validator = PromptValidator()\n",
    "\n",
    "# Validate all responses\n",
    "test_cases = [\n",
    "    (\"Chain-of-Thought\", cot_response, [\"step\", \"analysis\", \"churn\"]),\n",
    "    (\"Role-Based\", role_response, [\"architecture\", \"scalable\", \"recommend\"]),\n",
    "    (\"Template-Based\", template_response, [\"subject\", \"issue\", \"actions\"]),\n",
    "    (\"Step-by-Step\", step_response, [\"contain\", \"assess\", \"recover\"])\n",
    "]\n",
    "\n",
    "scores = []\n",
    "for name, response, expected in test_cases:\n",
    "    metrics = validator.validate_response(response, expected)\n",
    "    score = validator.calculate_score(metrics)\n",
    "    scores.append(score)\n",
    "    print(f\"{name}: {score:.2f} score\")\n",
    "\n",
    "avg_score = sum(scores) / len(scores)\n",
    "print(f\"\\nAverage pattern score: {avg_score:.2f} ({avg_score*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311aad0c",
   "metadata": {},
   "source": [
    "## A/B Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866bad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptABTester:\n",
    "    def __init__(self, validator):\n",
    "        self.validator = validator\n",
    "        self.results = []\n",
    "    \n",
    "    async def run_test(self, agent, prompt_a, prompt_b, test_name, expected=None):\n",
    "        response_a = await agent.run(prompt_a)\n",
    "        response_b = await agent.run(prompt_b)\n",
    "        \n",
    "        score_a = self.validator.calculate_score(self.validator.validate_response(response_a, expected))\n",
    "        score_b = self.validator.calculate_score(self.validator.validate_response(response_b, expected))\n",
    "        \n",
    "        improvement = ((score_b - score_a) / score_a) * 100 if score_a > 0 else 0\n",
    "        \n",
    "        result = {\n",
    "            \"test\": test_name,\n",
    "            \"score_a\": score_a,\n",
    "            \"score_b\": score_b,\n",
    "            \"improvement\": improvement\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        \n",
    "        print(f\"\\n{test_name}:\")\n",
    "        print(f\"A: {score_a:.3f}, B: {score_b:.3f}, Improvement: {improvement:+.1f}%\")\n",
    "        return result\n",
    "\n",
    "ab_tester = PromptABTester(validator)\n",
    "\n",
    "# Test 1: Basic vs Chain-of-Thought\n",
    "basic = \"Our app has 2.1 stars. Users complain about crashes. Create action plan.\"\n",
    "enhanced = patterns.chain_of_thought(\"App has 2.1 stars, crashes reported\", \"Mobile app performance issues\")\n",
    "\n",
    "await ab_tester.run_test(test_agent, basic, enhanced, \"Basic vs CoT\", [\"action\", \"plan\", \"crashes\"])\n",
    "\n",
    "# Test 2: Generic vs Role-Based  \n",
    "generic = \"Choose between React Native, Flutter, or native. Recommend best option.\"\n",
    "role_enhanced = patterns.role_based(\"Choose mobile framework\", \"Mobile Architect\", \"Cross-platform expertise\")\n",
    "\n",
    "await ab_tester.run_test(test_agent, generic, role_enhanced, \"Generic vs Role\", [\"react\", \"flutter\", \"recommend\"])\n",
    "\n",
    "# Calculate summary\n",
    "improvements = [r[\"improvement\"] for r in ab_tester.results]\n",
    "avg_improvement = sum(improvements) / len(improvements)\n",
    "print(f\"\\nAverage improvement: {avg_improvement:+.1f}%\")\n",
    "\n",
    "if avg_improvement >= 20:\n",
    "    print(\"Excellent: 20%+ consistent gains!\")\n",
    "else:\n",
    "    print(\"Good: Meaningful improvements shown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3952017",
   "metadata": {},
   "source": [
    "## Production Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbfeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionLibrary:\n",
    "    def __init__(self):\n",
    "        self.patterns = ProvenPromptPatterns()\n",
    "        self.usage = {\"cot\": 0, \"role\": 0, \"template\": 0, \"step\": 0}\n",
    "    \n",
    "    def get_best_pattern(self, task_type):\n",
    "        mapping = {\n",
    "            \"analysis\": \"cot\",\n",
    "            \"expert\": \"role\", \n",
    "            \"communication\": \"template\",\n",
    "            \"process\": \"step\"\n",
    "        }\n",
    "        return mapping.get(task_type, \"cot\")\n",
    "    \n",
    "    def create_prompt(self, task_type, **kwargs):\n",
    "        pattern = self.get_best_pattern(task_type)\n",
    "        self.usage[pattern] += 1\n",
    "        \n",
    "        if pattern == \"cot\":\n",
    "            return self.patterns.chain_of_thought(kwargs.get(\"problem\", \"\"), kwargs.get(\"context\", \"\"))\n",
    "        elif pattern == \"role\":\n",
    "            return self.patterns.role_based(kwargs.get(\"task\", \"\"), kwargs.get(\"role\", \"\"), kwargs.get(\"expertise\", \"\"))\n",
    "        elif pattern == \"template\":\n",
    "            return self.patterns.template_based(kwargs.get(\"content\", \"\"), kwargs.get(\"format\", \"\"))\n",
    "        elif pattern == \"step\":\n",
    "            return self.patterns.step_by_step(kwargs.get(\"goal\", \"\"), kwargs.get(\"process\", \"\"))\n",
    "\n",
    "library = ProductionLibrary()\n",
    "\n",
    "# Test automatic pattern selection\n",
    "scenarios = [\n",
    "    (\"analysis\", \"Market analysis task\"),\n",
    "    (\"expert\", \"Technical architecture\"), \n",
    "    (\"communication\", \"Customer email\"),\n",
    "    (\"process\", \"Security response\")\n",
    "]\n",
    "\n",
    "print(\"INTELLIGENT PATTERN SELECTION:\")\n",
    "for task_type, description in scenarios:\n",
    "    recommended = library.get_best_pattern(task_type)\n",
    "    print(f\"{description}: {recommended}\")\n",
    "\n",
    "# Generate some example prompts\n",
    "market_prompt = library.create_prompt(\n",
    "    \"analysis\", \n",
    "    problem=\"Competitor launched at 50% lower price\",\n",
    "    context=\"Enterprise B2B market\"\n",
    ")\n",
    "\n",
    "tech_prompt = library.create_prompt(\n",
    "    \"expert\",\n",
    "    task=\"Design rate limiting for 1M requests/hour\", \n",
    "    role=\"Backend Engineer\",\n",
    "    expertise=\"Microservices, Redis\"\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated market prompt: {len(market_prompt)} chars\")\n",
    "print(f\"Generated tech prompt: {len(tech_prompt)} chars\")\n",
    "print(f\"\\nUsage stats: {library.usage}\")\n",
    "print(\"Production library working perfectly!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
